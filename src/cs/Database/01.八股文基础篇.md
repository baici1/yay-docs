---
title: 八股文基础篇
date: 2022-02-12 17:19:55
category: 
  - 计算机基础
  - 数据库
tag: 
  - null
author: 
  name: 团子
  url: https://github.com/baici1
comment: false
description: null
icon: null
isOriginal: true
star: false
article: true
timeline: true
image: null
banner: null
---
# 基础篇

## 基础架构

***当你准备学习源码部分时候，不能陷入一个细节中，需要先看到全貌。***

把 `Mysql` 进行拆解一下，当执行一条语句时候，到底会经历哪些过程？

```mysql
select * from T where ID=10；
```

`Mysql` 架构示意图，来此 `geekTime`。

![img](https://static001.geekbang.org/resource/image/0d/d9/0d2070e8f84c4801adbfa03bda1f98d9.png)

从上图可以看出，`Mysql` 可以分为两层：`Server` 层和存储引擎层。

`Server` 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 `MySQL` 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层负责数据的存储和读取，以插件形式，支持 `InnoDB、MyISAM、Memory` 等多个存储引擎。

### 连接器

当你作为客户端，打算来使用 `Mysql` 时候，首先第一件事就是连接，连接器负责与客户端连接，获取权限，维持和管理连接。当你使用 `navicat` 连接某个服务器数据库，都会输入地址，用户名，密码，其实执行的连接命令如下：

```shell
mysql -h$ip -P$port -u$user -p
```

客户端与服务端的 `Mysql` 通过 `TCP` 连接后，连接器根据用用户名和密码就会开始验证你的身份。

* 如果用户名或密码不对，你就会收到一个"`Access denied for user`"的错误，然后客户端程序结束执行。
* 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。

当客户端连接完成后，没有后续操作，太长时间没有动静，连接器就会自动将它断开，这个时间由参数 `wait_timeout` 控制，默认时间是 `8` 小时。

当此时连接已经断开了，你客户端再次发送请求，就会收到错误提醒： `Lost connection to MySQL server during query。`，这个时候你要使用，就需要重连，然后再次发送请求。

在使用过程中，为了尽量减少连接的过程，尽量使用长连接，但是这里有一个问题存在：**使用长连接，有些临时内存存储的资源就会不断地变多，长期下去，就会内存占用太大，被系统强行杀掉（OOM），从现象看就是 `MySQL` 异常重启了。但是这些资源只能在连接断开时候释放，那么应怎么解决呢？**

考虑两种方案：

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
2. 如果你用的是 `MySQL 5.7` 或更新版本，可以在每次执行一个比较大的操作后，通过执行  `mysql_reset_connection` 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

### 查询缓存

`MySQL 8.0` 版本直接将查询缓存的整块功能删掉了，也就是说 `8.0` 开始彻底没有这个功能了。虽然之前版本存在但是使用频率很小，而且弊大于利，所以就不多介绍这一部分了。

### 分析器

`Mysql` 需要对 `SQL` 语句进行解析。过程如下

* 分析器先会作**词法分析**。将你输入地语句拆分成多个字符串，识别字符串地具体含义。
* 接着做**语法分析**，语法分析器会根据语法规则，判断你输入地 `SQL` 语句是否满足 `Mysql` 语法。如果你的语句不对，就会收到错误提醒：`You have an error in your SQL syntax`。

如果你处理 bug 多了，就会发现一般提示错误地位置都是在 `use near` 后面地内容。

### 优化器

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。

例如以下语句：

```mysql
select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;
```

* 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。

* 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。

这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

### 执行器

开始执行前，会首先判断当前用户是否有权限去做这件事情，如果没有，就会返回没有权限地错误。

打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。执行流程如下：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

至此，这个语句就执行完成了。

## 日志系统

有一个说法：`MySQL` 可以恢复到半个月内任意一秒的状态。太神奇了！！

我们从一个表的更新语句开始：

```mysql
update T set c=c+1 where ID=2;
```

基本上执行流程与上面的查询语句一致，但是不一样的是，更新流程还涉及到两个重要的日志模块：`redo log`（重做日志）和 `binlog`（归档日志）。

### redo log

当我们想要恢复到之前的一个状态，如果每次的更新操作都要写进磁盘，然后通过磁盘找到记录，然后再更新，这一个过程的 `IO` 成本和查询成本就比较高。一般采用的方式都是先写日志，然后再写进磁盘。

具体的过程：当一条记录需要更新时候，引擎会先把记录写在日志中，并更新内存，引擎会再适当时候（空闲状态/日志空间满了）将操作记录更新到磁盘中。

![img](https://static001.geekbang.org/resource/image/16/a7/16a7950217b3f0f4ed02db5db59562a7.png)

`write pos` 是当前记录的位置，`check point` 是当前要擦除的位置，不断往后推移与循环。

`write pos`和`check point` 之间的空白区域，是可以用来记录新的操作记录的，如果当前 `write pos` 追上了 `check point` 那么代表当前日志已满，不能再记录了，需要停下来，擦除一些记录。

有了 `redo log`，`InnoDB`就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**`crash-safe`**。

### binlog

`Mysql` 是由两部分组成，`redo log` 其实是 `InnoDB` 引擎特有的日志。`binlog` 负责 `serve` 层。

`binlog` 会记录所有的逻辑操作，采用追加写的方式。

> 咦！🤷‍♀️为什么会有两个日志，一个日志不够吗？

这两种日志有以下三点不同。

1. `redo log` 是 `InnoDB` 引擎特有的；`binlog` 是 `MySQL` 的 `Server` 层实现的，所有引擎都可以使用。
2. `redo log` 是物理日志，记录的是“在某个数据页上做了什么修改”；`binlog` 是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
3. `redo log` 是循环写的，空间固定会用完；`binlog` 是可以追加写入的。“追加写”是指 `binlog` 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

mysql 同时需求归档（记录所有的操作记录）和重做（回到之前的状态）的功能。

我们接下来看看，这两个日志在更新语句的流程中会怎样进行提交呢？

![img](https://static001.geekbang.org/resource/image/2e/be/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png)

1. 执行器先找引擎取 `ID=2` 这一行。ID是主键，引擎直接用树搜索找到这一行。如果 `ID=2` 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 `N`，现在就是 `N+1`，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 `redo log` 里面，此时 `redo log` 处于 `prepare`状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 `binlog`，并把 `binlog` 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 `redo log` 改成提交（`commit`）状态，更新完成。

> 为什么redo log 没有先提交，而是采用事务方式，两个日志一起提交呢？

答案：为了让两份日志之间逻辑一致。

为什么这样子做可以做到逻辑一致呢？我们要回到之前的问题，**`MySQL` 怎么做到恢复到半个月内任意一秒的状态？**

谈到恢复文件过程：备份系统中会保存最近半个月的所有 `binlog`，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

* 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
* 然后，从备份的时间点开始，将备份的 `binlog` 依次取出来，重放到中午误删表之前的那个时刻。

你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

好像不能解释第一个问题，但是这里提醒了，逻辑一致的重要性。如果逻辑性不一致，那么你恢复的数据库就会产生偏差。

为什么日志需要事务方式提交？通过**反证法**来证明：

如果 redo log 和 binlog 提交存在顺序会发生什么情况呢？

假设当前 `ID=2` 的行，字段 `c` 的值是 0，再假设执行 `update` 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 `crash`，会出现什么情况呢？

1. **先写redo log后写binlog**。假设在 `redo log` 写完，`binlog` 还没有写完的时候，`MySQL` 进程异常重启。由于我们前面说过的，`redo log` 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。
   但是由于 `binlog` 没写完就 `crash`了，这时候 `binlog` 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 `binlog` 里面就没有这条语句。
   然后你会发现，如果需要用这个 `binlog` 来恢复临时库的话，由于这个语句的 `binlog` 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 `c` 的值就是0，与原库的值不同。
2. **先写binlog后写redo log**。如果在 `binlog` 写完之后 `crash`，由于 `redo log` 还没写，崩溃恢复以后这个事务无效，所以这一行 `c` 的值是 0。但是 `binlog` 里面已经记录了“把c从0改成1”这个日志。所以，在之后用 `binlog` 来恢复的时候就多了一个事务出来，恢复出来的这一行 `c` 的值就是 1，与原库的值不同。

如果不通过事务方式提交，数据库源库的状态就有可能与用日志恢复的出现不一样的。

有人会杠一下：这个发生的概率是不是低了点？

数据库存储数据的，首先不允许发生错误（例如银行的数据库，发生了错误，你的钱少了你会干吗？），其次不一定备份的使用场景在恢复数据库，可以出现数据库扩容情况。

简单说，`redo log` 和 `binlog` 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

## 事务

事务，其实不陌生，最经典的例子：银行转账。

简单来说，事务是要保证当前一串的操作，要么全部成功，要么全部失败，在 `Mysql` 中事务支持在引擎层完成。

### 隔离性与隔离级别

事务有四个特性：ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），我们来讨论一下其中的隔离性。

当数据库多个事务同时执行时候，就有可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了隔离级别的概念。

首先需要知道，隔离越严实，效率就会越低。很多时候，需要找到两者的平衡点。

SQL标准的事务隔离级别包括：

* 读未提交（read uncommitted）是指一个事务还没提交时，它做的变更就能被别的事务看到。
* 读提交（read committed）是指一个事务提交之后，它做的变更才会被其他事务看到。
* 可重复读（repeatable read）是指一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
* 串行化（serializable ）是指“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

举个例子来说明这四个事务隔离级别：

```mysql
create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

![img](https://static001.geekbang.org/resource/image/7d/f8/7dea45932a6b722eb069d2264d0066f8.png)

* 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。
* 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。
* 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。
* 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。

不同的隔离级别下，数据库行为采取的读取数据方式都是不同的：

* 若隔离级别是“读未提交”，**直接返回记录上的最新值。**
* 若隔离级别是“读提交”，会在**每个 `sql` 语句开始执行**的时候进行**创建视图**，读取视图中的数据。
* 若隔离级别是“可重复读”，则是在**事务启动的时候创建视图**，那么在这个过程中数据都是此视图提供
* 若隔离级别是“串行化”，通过加锁方式进行访问数据，避免并行访问。

每种隔离级别都是有自己的使用场景，根据自己业务情况去使用。

### 事务隔离实现

在 `MySQL` 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。

![img](https://static001.geekbang.org/resource/image/d9/ee/d9c313809e5ac148fc39feff532f0fee.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 `read-view`。回滚日志，系统会判断，当没有事务回需要这些回滚日志时候，就会被删除。

> 为什么建议你尽量不要使用长事务？

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。

### 事务启动方式

`MySQL`的事务启动方式有以下几种：

1. 显式启动事务语句， `begin` 或 `start transaction`。配套的提交语句是 `commit`，回滚语句是`rollback`。
2. `set autocommit=0`，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 `select` 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 `commit` 或 `rollback` 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个 `set autocommit=0` 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

**因此，我会建议你总是使用 `set autocommit=1`, 通过显式语句的方式来启动事务。**

但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 `commit work and chain` 语法。

在 `auto commit`为1 的情况下，用 `begin` 显式启动的事务，如果执行 `commit` 则提交事务。如果执行 `commit work and chain`，则是**提交事务并自动启动下一个事务**，这样也省去了再次执行 `begin` 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

你可以在 `information_schema` 库的 `innodb_trx`这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

```mysql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

## 索引

***索引的出现其实就是为了提高数据查询的效率。***

### 索引的常见模型

有三种常见也比较简单的数据结构，可以实现索引的：**哈希表**，**有序数组**，和**搜索树**。

> 哈希表

哈希表是一种以键-值（`key-value`）存储数据的结构，我们只要输入待查找的键即 `key`，就可以找到其对应的值即 `Value`。哈希的思路很简单，把值放在数组里，用一个哈希函数把 `key` 换算成一个确定的位置，然后把 `value` 放在数组的这个位置。

当出现多个 key 值经过哈希函数换算，出现同一个值的情况，处理这种情况的方法是**拉链法**。

举个例子🌰：

你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示：

![img](https://static001.geekbang.org/resource/image/0c/57/0c62b601afda86fe5d0fe57346ace957.png)

当你想要查询 `name2` 时候，首先将 `ID_card_n2` 通过哈希算法算出 `N`，然后通过链表一次遍历，最后找到对应值。

好处：增加新的User时速度会很快，只需要往后追加。

缺点：因为不是有序的，所以哈希索引做区间查询的速度是很慢的。当你做区间查询，你就需要将区间内的所有用户都要扫描一遍。

所以，**哈希表这种结构适用于只有等值查询的场景**，比如 `Memcached` 及其他一些 `NoSQL` 引擎。

> 有序数组

举个例子🌰：

![img](https://static001.geekbang.org/resource/image/bf/49/bfc907a92f99cadf5493cf0afac9ca49.png)

这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。

等值查询：利用二分法查询 `ID_card_n2` 对应的名字，时间复杂度为 `O(logN)`。

区间查询：两次二分法分别查询左边界与右边界，就可以获得区间内的所有用户。

**有序数组在等值查询和范围查询场景中的性能就都非常优秀**。但是当你需要更新数据时候，就会显得很麻烦，往中间插入一条记录就得挪动后面所有的记录位置，成本太高了。

所以，**有序数组索引只适用于静态存储引擎**，比如你要保存的是2017年某个城市的所有人口信息，这类不会再修改的数据。

> 二叉搜索树

二叉搜索树的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。这样如果你要查 `ID_card_n2` 的话，按照图中的搜索顺序就是按照 `UserA -> UserC -> UserF -> User2` 这个路径得到。这个时间复杂度是 `O(log(N))`。同时更新数据的时间复杂度是 `O(log(N))`。

好像与上面两个比起来，二叉搜索树的查询与更新效率都是优秀的。实际上大多数的数据库存储却并不会使用二叉树，因为索引不止存在内存中，还要写在磁盘中。为了让一个查询尽量少的读磁盘，不使用二叉树，而是使用 `N` 叉树，这里，`N`叉”树中的 `N` 取决于数据块的大小。

### InnoDB 的索引模型

在 `InnoDB` 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。`InnoDB` 使用了`B+` 树索引模型，所以数据都是存储在 `B+` 树中的。每一个索引在 `InnoDB` 里面对应一棵 `B+` 树。

举个🌰：

我们有一个主键列为 `ID` 的表，表中有字段 `k`，并且在 `k` 上有索引。(ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5)和(600,6)。

![img](https://static001.geekbang.org/resource/image/dc/8d/dcda101051f28502bd5c4402b292e38d.png)

根据叶子节点的内容，索引的类型分为主键索引和非主键索引。

主键索引的叶子节点存的是**整行数据**。在 `InnoDB` 里，主键索引也被称为聚簇索引（`clustered index`）

非主键索引的叶子节点内容是主键的值。在 `InnoDB` 里，非主键索引也被称为二级索引（`secondary index`）。

我们来讨论一个问题：**基于主键索引和普通索引的查询有什么区别？**

假设我们都要查询 `ID=500` 的行的数据：

* 主键查询方式：`select * from T where ID=500`，则只需要搜索 `ID` 这棵 `B+` 树。
* 普通索引查询方式：`select * from T where k=5`，需要先搜索 `k` 索引树，得到 `ID` 的值为 `500`，再到 `ID` 索引树搜索一次。这个过程称为回表。

所以通过上述情况，**基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。**

### 索引维护

`B+` 树为了维护索引有序性，在插入新值时候需要做必要的维护。以上面这个图为例，如果插入新的行 `ID` 值为700，则只需要在 `R5` 的记录后面插入一个新记录。如果新插入的ID值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果 R5 所在的数据页已经满了，根据 `B+` 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。

除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。

当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。

> 我们讨论一个问题：为什么每次建表里面需要有一个自增主键。哪些场景下使用自增主键呢，哪些常见不使用呢？

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： `NOT NULL PRIMARY KEY AUTO_INCREMENT`，插入新记录的时候可以不指定 `ID` 的值，系统会获取当前 `ID` 最大值加 1 作为下一条记录的 `ID` 值。

从性能方面讲：自增主键插入数据，都是追加操作，不会挪动其他记录，也不会触发叶子节点的分裂，性能上会高出一些。

从存储空间来看：假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，虽然身份证可以作为主键使用，但是如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是8个字节。

所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。

场景需求：

* 只有一个索引；
* 该索引必须是唯一索引。

在此场景下使用自增主键是合理的。

## 索引的使用

有这样一个问题：

在下面这个表 `T` 中，如果我执行 `select * from T where k between 3 and 5`，需要执行几次树的搜索操作，会扫描多少行？

```mysql
mysql> create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;

insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
```

![img](https://static001.geekbang.org/resource/image/dc/8d/dcda101051f28502bd5c4402b292e38d.png)

具体流程：

* 首先在 `k` 索引树找到 `K=3` 的记录，取得 `id=300` 的记录。
* 然后再去 `id` 索引树 找到 `id=300` 的对应的 `R3`.
* 重复以上操作，找到 `k=5` 对应的 `R4` 的记录。
* 在 `k` 索引树取下一个值 `k=6`，不满足条件，循环结束。

**回到主键索引树搜索的过程，称之为回表。**在上诉的查询过程中，经历了两次回表，为了优化性能，有没有方法避免回表过程呢？

### 覆盖索引

如何避免回表？，最直接想法，不产生回表，`select ID from T where k between 3 and 5` 查询 `id` 的值似乎不会产生回表过程。索引树 `k` 其实他已经覆盖了我们的查询需求，把这称之为**覆盖索引**。

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**

有一个注意点：在引擎内部使用覆盖索引在索引 K 中 其实读了 3 个数据：300~600，但是对于 `MySQL` 的 `Server`层来说，只是拿到了两条数据，所以 `MySQL` 认为扫描的行数是 2。

接下来根据覆盖索引讨论一个问题：**在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？**

市民表定义：

```mysql
CREATE TABLE `tuser` (
  `id` int(11) NOT NULL,
  `id_card` varchar(32) DEFAULT NULL,
  `name` varchar(32) DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `ismale` tinyint(1) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `id_card` (`id_card`),
  KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB
```

我们知道身份证号是市民的唯一标识，可以根据身份证号查询到他的相关信息，他作为主键建立索引就好了，什么情况下我需要建立一个（身份证与姓名）联合索引呢？

**如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。**，建立了索引，你也需要维护，是否需要建立冗余的联合索引支持覆盖索引，就需要多权衡一下了。

覆盖索引的目的仅仅是不回表。

### 最左前缀原则

每次查询都要建立一个覆盖索引？这就会导致出现过大的维护代价，但是我只是根据身份证号去查询家庭地址字段，出现概率不高，总不能全表查询吧，查询的代价也比较大，可以建立一个覆盖索引很浪费啊。怎么做呢？看看下面这个解决方案。

**B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。**

![img](https://static001.geekbang.org/resource/image/89/70/89f74c631110cfbc83298ef27dcd6370.jpg)

以 (name,age) 联合索引举例：

我们可以发现，索引项是按照索引定义里面出现的字段顺序排序的。

有一个需求：查询所有名字为 张三 的人信息，这样就可以快速定位到 `ID-4`，然后往后继续遍历，得到结果。

有一个需求：查询的是所有名字第一个字是“张”的人，你的 `SQL` 语句的条件是"`where name like ‘张 %`’"。，你依旧可以用上这个索引，找到第一个满足条件的记录是 `ID-3` ，往后遍历，得到结果。

不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个**最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。**

在上诉的前提下：如果查询条件是 `where name like ‘%张 %‘`，其实就没有用到索引。同时当你建立了`(name,age)`  ，其实相当于你也建立了 `name` 的普通索引。

还有一个问题：**在建立联合索引时候，应该如何安排索引内字段顺序？**

评估标准是索引的复用能力。联合索引(A, B)意味着不需要建立A的索引了，因为这个联合索引意味着建立了(A,B)和(A)这两种索引，因此**第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。**

现在就可以将前面两个问题做一个综合，建立（身份证号，姓名）联合索引可以解决**根据身份证号查询姓名高频请求**，以及**解决根据身份证号查询地址**的需求。

如果现在有联合查询`（a，b）`，又有基于 `a,b` 的各自查询，根据最左前缀原则，是无法使用 `(a,b)` 这个联合索引，如果你又建立一个 `b` 的普通索引，需要同时维护 `(a,b)、(b)` 这两个索引。

这里需要考虑的原则就是空间问题，你建立越多的索引，就需要更多的存储空间。如果你需要维护两个索引，最好根据**存储空间**，确定两者的索引顺序，比如上面这个市民表的情况，选择 `name` 字段放在左边，再建立一个 age 字段普通索引。

### 索引下推

说到一个问题：**最左前缀可以用于在索引中定位记录。如果不符合最左前缀的部分，又会怎么样呢？**

以市民表的联合索引`（name, age）`为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。

```mysql
select * from tuser where name like '张%' and age=10 and ismale=1;
```

根据前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。

> 💡:
>
> mysql 会一直向右匹配直到遇到范围查询（>、<、between、like）就停止匹配。范围列可以用到索引，但是范围列后面的列无法用到索引。即，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。

继续判断其他条件是否满足。

在 `MySQL 5.6` 之前，只能从 `ID3` 开始一个个回表。到主键索引上找出数据行，再对比字段值。

![img](https://static001.geekbang.org/resource/image/b3/ac/b32aa8b1f75611e0759e52f5915539ac.jpg)

而 `MySQL 5.6` 引入的索引下推优化（`index condition pushdown`)， 可**以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。**

![img](https://static001.geekbang.org/resource/image/76/1b/76e385f3df5a694cc4238c7b65acfe1b.jpg)

这两个过程的区别是：

* MySQL 5.6 之前不会去看索引中 age 的值，就会将按顺序把“name 第一个字是’张’”的记录一条条取出来回表，匹配条件，回表 4 次。
* MySQL 5.6 之后，会先判断索引中内部的值，对于 `age!=10` 的记录，就会直接判断并跳过，满足 `age=10` 的则会回表取数据进行再一次的判断。

总结：

1. 覆盖索引：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少IO磁盘读写读取正行数据
2. 最左前缀：联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符
3. 联合索引：根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。
4. 索引下推：like 'hello%’and age >10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度

## 全局锁和表锁

根据加锁的范围，MySQL 里面的锁大致分为全局锁，表级锁和行锁三类。

### 全局锁

全局锁就是对整个数据库实例加锁。`MySQL` 提供了一个加全局读锁的方法，命令是 `Flush tables with read lock` (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

**全局锁的经典使用场景是，做全库逻辑备份。**也就是把每个表都查询处理，做出文本，进行备份处理。

以前有一种做法，是通过 `FTWRL` 来确保不会有其他线程对数据库做更新，然后对整个库完成**备份需求**。注意，在备份过程中整个库完全处于只读状态。

但是让整库都只读，听上去就很危险：

* 如果你在主库上备份，那么**在备份期间都不能执行更新，业务基本上就得停了**；
* 如果你在从库上备份，那么**备份期间从库不能执行主库同步过来的 `binlog`，会导致主从延迟。**

备份为什么需要加锁？不加锁会有什么情况出现呢？

🌰：

假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。

现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。如果时间顺序上是先备份账户余额表 (u_account)，然后用户购买，然后备份用户课程表 (u_course)，会怎么样呢？

![img](https://static001.geekbang.org/resource/image/cb/cd/cbfd4a0bbb1210792064bcea4e49b0cd.png)

备份结果是：用户 A 的**余额没有发生变化，但是课表多了一门课！**如果后面用这个备份恢复数据，对于用户 A 来说就赚了。如果先备份用户课程表再备份用户余额表，对于用户 A 来说出现的结果就是**课程没有发生变化，但是余额却扣了。**

不加锁的话，备份系统备份的**得到的库不是一个逻辑时间点（因为不同表之间的执行顺序不同进而备份的时间不同），**这个视图是逻辑不一致的。

说到视图，**如果视图是一致的，那么是不是也可以完成备份数据的功能？**

之前说过，在可重复读隔离级别下开启事务，可以获得事务过程中一致的视图。

官方自带的逻辑备份工具是 `mysqldump`。当 `mysqldump` 使用`参数–single-transaction` 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 `MVCC` 的支持，这个过程中数据是可以正常更新的。

**有了这个功能，那为什么还需要 `FTWRL` 呢？**

答：确实能够完成，但是有些引擎不支持这个事务，例如 MyISAM。所以更加大众化的选择就是  `FTWRL` 命令来做全库备份。

**通过使用 `set global readonly=true` 的方式，应该也可以使全库处于只读状态，那为什么不使用呢？**

原因：

* 在有些系统中，`readonly` 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 `global` 变量的方式**影响面更大**，我不建议你使用。
* 在异常处理机制上有差异。如果执行 `FTWRL` 命令之后由于客户端发生异常断开，那么 `MySQL` 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 `readonly` 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，**风险较高**。

### 表级锁

`MySQL` 里面表级别的锁有两种：一种是表锁，一种是元数据锁。

表锁的语法是 `lock tables … read/write` ,可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。

* 对表加**读锁**后，自己也不能对其进行修改；自己和其他线程只能读取该表。

* 当对某个表执加上**写锁**后，该线程可以对这个表进行读写，其他线程对该表的读和写都受到阻塞；

总结：**写锁是独占的，读锁是共享的**。

在还没有出现更细粒度的锁的时候，**表锁是最常用的处理并发的方式**。而对于 `InnoDB` 这种支持行锁的引擎，一般不使用 `lock tables` 命令来控制并发，**毕竟锁住整个表的影响面还是太大。**

元数据锁是 `MDL（metadata lock)`。**MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。**

元数据锁是 `server` 层的锁，表级锁，主要用于隔离 `DML`（Data Manipulation Language，数据操纵语言，如select）和 `DDL`（Data Definition Language，数据定义语言，如改表头新增一列）操作之间的干扰。

每执行一条 `DML、DDL` 语句时都会申请 `MDL` 锁，`DML` 操作需要 `MDL` 读锁，`DDL` 操作需要 `MDL` 写锁（MDL加锁过程是系统自动控制，无法直接干预，读读共享，读写互斥，写写互斥）

* **读锁之间不互斥**，因此你可以有多个线程同时对一张表增删改查。
* **读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。**因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

🌰：

**有人掉到这个坑里：给一个小表加个字段，导致整个库挂了。**

给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。

* 索引要根据表中的每一行的记录值来创建，所以需要全表扫描；
* 加字段或修改字段，也要修改每一行记录中的对应列的数据，所以也要全表扫描）

在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，**即使是小表，操作不慎也会出问题。**

假设表 t 是一个小表。

![img](https://static001.geekbang.org/resource/image/7c/ce/7cf6a3bf90d72d1f0fc156ececdfb0ce.jpg)

`session A` 先启动，这时候会对表 t 加一个 `MDL` 读锁。由于 `session B` 需要的也是 MDL 读锁，读锁是可以共享的，因此可以正常执行。

之后 `session C` 需要 `MDL` 写锁，所以会被 `blocked`，在这里被阻塞了。

为什么 session D 也被锁住了呢？

如果只有 `session C` 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 `MDL` 读锁的请求也会被 `session C` 阻塞，**读写锁是互斥的**，那么换句话说，后面所有的增删改查的操作都会申请 MDL ，但是在 `session C` 后面都会被锁住。**导致整个表不可读写了。**

接下来，如果**某个表上的查询语句频繁，**而且客户端有重试机制，也就是说超时后会再起一个新 `session` 再请求的话，这个库的连接池爆满，从而导致整个库不能使用了。

基于上面的分析，我们来讨论一个问题，**如何安全地给小表加字段？**

1. **解决长事务，**事务不提交，就会一直占着 `MDL` 锁。具有严重威胁并发性，也会导致回滚段不能回收长时间被占用空间。如果你要做 `DDL` 变更的表刚好有长事务在执行，要考虑先暂停 `DDL`，或者 `kill` 掉这个长事务。
2. `alter table` 语句里面**设定等待时间，**如果在这个指定的等待时间里面能够拿到 `MDL` 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 `DBA` 再通过重试命令重复这个过程。

对于出现锁的解决方法：一般都是**防止死锁或者利用超时时间来解决长时间卡顿。**

总结：

全局锁**主要用在逻辑备份过程中**。对于全部是 `InnoDB` 引擎的库，**我建议你选择使用`–single-transaction` 参数，对应用会更友好。**

表锁一般是**在数据库引擎不支持行锁的时候才会被用到的。**如果你发现你的应用程序里有 `lock tables` 这样的语句，你需要追查一下，比较可能的情况是：

* 要么是你的系统现在还在用 `MyISAM` 这类不支持事务的引擎，那要安排升级换引擎；
* 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 `lock tables` 和 `unlock tables` 改成 `begin` 和 `commit`，问题就解决了。

`MDL` 会**直到事务提交才释放**，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

## 行锁

行锁就是针对数据表中行记录的锁。并不是所有引擎都支持行锁，比如 `MyISAM` 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁。`InnoDB` 是支持行锁的。

对于行锁来说：

* 锁粒度越大，开销越小，锁冲突的概率越小，安全性也就越高，但业务并发度以及性能越差；

* 反之锁粒度越小，开销也就越大，锁冲突的概率越大（易导致死锁），安全性也就越低，但业务并发度以及性能越好。

### 两阶段锁

🌰：在下面的操作序列中，事务 B 的 `update` 语句执行时会是什么现象呢？假设字段 id 是表 t 的主键。

![img](https://static001.geekbang.org/resource/image/51/10/51f501f718e420244b0a2ec2ce858710.jpg)

对于这个问题，需要考虑到事务 A 的情况，在执行 `update` 语句后，未提交，会持有哪些锁，同时什么时候释放。

问题的答案是：实际上事务 B 的 `update` 语句会被阻塞，直到事务 A 执行 `commit` 之后，事务 B 才能继续执行。

对于**两阶段锁**，**锁的添加与释放分到两个阶段进行**，之间不允许交叉加锁和释放锁。 也就是在**事务开始执行后为涉及到的行按照需要加锁，但执行完不会马上释放，而是在事务结束时再统一释放他们。**

两阶段锁协议，可以帮助，当**你的事务出现了锁多个行，需要把最有可能发生锁冲突，最可能影响并发度的锁尽量往后放。**

🌰：

假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作：

1. 从顾客 A 账户余额中扣除电影票价；
2. 给影院 B 的账户余额增加这张电影票价；
3. 记录一条交易日志。

要完成这次交易，需要 `update` 两条记录，并 `insert` 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。

首先，我们要明白**：事务在执行的时候，并不是一次性把所有行锁都持有，而是执行到哪一行就拿哪一行的锁。等到最后 `commit` 的时候，一起释放**。

同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。

根据上诉的条件，知道把冲突的语句 2 安排到最后，那么影院账户余额这一行的锁时间就最少，对于顾客 B,C 发生冲突的可能变小，最大程度减少了事务的锁的等待，提高了并发度。

### 死锁和死锁检测

**当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，**称为死锁。简单来讲：

![img](https://static001.geekbang.org/resource/image/4d/52/4d0eeec7b136371b79248a0aed005a52.jpg)

这个时候，事务 A 在等待事务 B 释放 id=2 的行锁，而对于事务 B 则在等待事务 A 释放 id=1 的行锁。因为锁都要等到 `commit` 才会释放，务 A 和事务 B 在互相等待对方的资源释放，就是进入了**死锁状态**。

解决死锁的有两种策略：

* 直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。`show variables like "%innodb_lock_wait_timeout%";`
* 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 `on`，表示开启这个逻辑。

在 `InnoDB` 中，`innodb_lock_wait_timeout` 的默认值是 `50s`，意味着如果采用第一个策略，当出现死锁以后，**第一个被锁住的线程要过 `50s` 才会超时退出，然后其他线程才有可能继续执行。**对于在线服务来说，这个等待时间往往是无法接受的。

如果时间设置的很小，例如 1s，可是这么短的时间，如果不是锁呢，只是简单锁等待，那么就会出现很多误伤。

综合看，第一种策略有很多的弊端。

正常情况下我们还是要采用第二种策略，即：**主动死锁检测**，而且 `innodb_deadlock_detect` 的默认值本身就是 `on`。**主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担（CPU占用）的。**

死锁检测：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

如果是我们上面说到的所有事务都要更新同一行的场景呢？

**每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。**假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 `CPU` 资源。因此，你就会看到 `CPU` 利用率很高，但是每秒却执行不了几个事务。

问题最后要归结到，死锁检测会耗费大量的 `CPU` 资源。

解决的策略有以下几种：

1. **视而不见**。我觉得业务不会出现，就不开启死锁检测。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。**而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。**
2. **控制并发**。只允许同时处理并发5个程序，也可以进一步解决，这个并发控制要做在**数据库服务端**。如果你有中间件，可以考虑在**中间件**实现；
3. **拆行处理**。将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。
